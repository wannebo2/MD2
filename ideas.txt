This document is perhaps rather ambitious and incoherent at the moment, but right now here are my thoughts on the model architecture:

Architecture plan, V2 (6-16-2025)

Current goals:
- Simulate DNA - protein interactions
- At an atomic level, and in different solutions (with different small molecules present, and at different temperatures)
- Be stable and accurate (with the correct probability distribution, not specific trajectory) for large timescales
- Be generalizable to systems significantly larger than what it is trained on
- Be able to take in bonding information (topology), and the prediction should maintain proper topology
- Be able to reference a database of known trajectories to obtain a more accurate prediction
- Produce a confidence estimate (train model to predict it's loss as well)

Plan:
- Use linear-time transformer variant directly on atoms, with atomic embedding (just one-hot?), and relative positions, so that it is both scalable and accurate
- Use atomic relative position/rotation embeddings, with the "direction" the atom is facing being the average bond vector? (also use relative temporal embedding)
- Distinguish between bonded and non-bonded atoms by either having a bondedness embedding concatenated like the position embeddings, or having some attention heads that only attend to bonded, or non bonded atoms. I like the bondedness embedding best currently.
- Use performer transformer, possibly with learned features instead of random ones (I need to examine that architecture some more)
- Have an attention layer that attends to hidden layer embeddings from other examples, and make sure to backpropagate gradient through that attention module as well
- Use the monarch layer where applicable (reduces parameter count and training time), and 8-bit Galore training (reduces training memory usage (and perhaps training time? I'll have to revisit the paper)), to try to make things as efficient as possible
- Attend to all given frames to diffuse atom coordinates (but not bondedness) in a future frame
- Noise should be fairly sensible (rigid bonds should not be stretched, but noise also must not introduce bias)
- Train on mostly DNA/Protien systems, but throw some other systems in the training set too, because there should be plenty of other MD data avalible, and it could help with generalization, even within DNA/protein systems
- Train the same model to predict confidence, and use bootstraping procedures described below to train the model beyond the MD dataset
- When training via confidence, the model being trained will need to specifically be trained to predict the same confidence as previous versions of the model, to avoid the confidence scores being effected by 'catasrophic forgetting'
- If we run into issues with the confidence system above, then perhaps it should be moved to it's own model



V1 below (V2 should be much simpler than this (and therefore easier to implement and probably more likley to work), and probably faster too)

What are the goals for the model?
- much more efficient than MD
- accurate at the scale(s) it operates at
  - accurate for what metrics? generating precisely the same trajectory from the same initial state seems both difficult and unnecessary. 
  - it would be nice (but not necessary) to preserve time-dependent processes
- able to be trained on small and short simulations and then operate on larger ones
  - it would be awesome if the timestep could be chosen at generation time, and if the model output could be used to bootstrap the training data for longer times
- able to generalize to
  - new DNA sequences
  - new proteins
  - new thermodynamic ensembles??? (ex: different salinities?)


Needs:
- must be trainable on the hardware we have access to in a reasonable time
- must be accurate enough to be useful
- must be faster than MD
- must be able to simulate at least 5 bp long dna sequences in the vicinity of small stationary proteins
- model output must be translationally/rotationally equivariant

Wants:
- scalability in all forms
- stability over long timescales
- able to simulate both single and double stranded dna
- accurately capture long-range (electrostatic) interactions
- accurate transition paths/probabilities
- able to run trained model on my laptop

Nice to haves:
- able to output a perhaps less accurate simulation with a larger timestep, and train this from smaller timestep
- able to use additional data without retraining (attention over dataset, it would be SO COOL and could really help with generalization, but the model might end up bigger and/or slower if we do this.)
- able to simulate at different coarse-grained resolutions (stretch goal, and I don't really see how we would do this at the moment without significantly increasing the number of parameters)
- able to perform other tasks, such as interpolating and molecule design (would go well with a diffusion model, see "Generative Modeling of Molecular Dynamics Trajectories")
- able to handle unseen compounds to some extent, or be able to be fine-tuned to handle a new compound without everything breaking.
- able to simulate long sequences of DNA in funky configurations (ex: part of origami construct)
- able to simulate the protein as well (probably not something we should focus on, but it would be nice)
- able to be trained on my laptop (without compromising efficiency)? (stretch goal) (perhaps we could train littler versions like they do with LLMs)


Information flow (very much subject to change):
1. Take PBDs and DCDs, get residue orientations and positions
2. Have each residue assotiated with a learnable embedding (intialize to one-hot plus random component?), which the network sees, and concatenate equivariant positional information to each embedding
3. Train transformer-based VAE on induvidual frames (or small sets of nearby frames??? it is simpler to just take in one frame, but then velocity information may need to be included seperately) of simulation data to
for N number of layers:
 - for each residue, take in residue embedding, along with (concat? or add?) embeddings and appropriatley transformed positions from nearby residues, and put them through standard transformer layer
 - Compute (weighted) average embeddings for groups of nearby residues (location w/ random offset, and output this to the next layer
Then, decode via using a simularly structured transformer to compute embeddings at some (partially randomized, at least for training) points in space.
Should output have zero/null embeddings where particles aren't? or should it be a continuous surface?
It might be easier to try to reconstruct a linear interlopation of nearby molocule identies as opposed to the discrete molocules themselves, at least with this setup.

The layers can probably be trained one at a time, starting with 1 input layer-1 output layer, then 2 input-layers, 1 output layer, ect. Should there only be one output layer? I don't feel like there needs to be a 1:1 correspondence between input and output layers, but being able to decompress the latent space iteratively like that could allow the multiple levels of corse-graining mentioned in "Nice to haves"

4. Once the frame-wise VAE is trained, a transformer diffusion model can be trained. If trying to do multi-scale thing that is seeming more and more plausible, then it could make sense to first train it to denoise only the most coarse layer, to learn the general 'shape' of the output, and then iterativley add the lower layers, and corresponding input/output neurons to the network. I think it makes most sense just to have one network denoising the entire (multi-level) output space.

Alternatley, it could make sense to train it instead on smaller portions of the input space 1st, like how the VAE was trained (and use smaller timesteps, too), and then move up to the more coarse-grained representations.

Model inputs:

A noisy version of the a few random VAE compressed frames from the DCD trajectory (along with relative time information),
potentially along with a random subset of KV pairs from past data instances (not the same one that it is being trained on)

Model outputs:

A less noisy version of the few random VAE compressed frames from the DCD trajectory,
potentially along with a new KV pair produced from this data instance

In between:

Just a basic equivariant transformer, I think.

Notes:
- If using past data KV pairs, should be a separate 'module' in the network, so that the model can operate without them, probably? Perhaps this module could be 'tacked on' after initial training if we decide we do want it. Old pairs must be forgotten as the model is trained, because the KV encoding will change and so training the model to be able to read old memories means it will have trouble learning to embed new ones. Probably only a random subset of KV pairs should be provided, so that the model knows how to operate when it doesn't have well-covered past experiences with the structures. Also, it is critical to make sure that memories from the past training on a trajectory are not avalible when training on that same trajectory. If using a large database of past experiences, then some sort of hashing mechanisim will be needed to efficiently recall the KV pairs actually relavent to the query.

- The noise should not be applied to all frames equally, because the primary goal is to predict future frames given past ones, so a significant amount of the training should be devoted to very noisy future frames paired with much less noisy past frames. Altenatley, some of the training could be spent only denoising part of the space, if only part has noise added in the first place. Also, different compression levels could be noised different amounts, or some just not provided, so that the model can be trained to operate on ' 

- The model should be trained with varying numbers of frames, and once short-time predictions are accurate, the model can be trained to make longer predictions by having predict a long trajectory by having it hallucinate a trajectory composed of many small frames, and then training it to denoise the frames at the very end of the long sequence given only the frames closer to the beginning of the sequence.
	- If the model is not accurate enough, this could lead to it reinforcing its inaccuracies, so this would be used to augment real data as opposed to replacing it.

- Different spatial sizes of frames should also be used, and this should work fine because the transformer architecture works just fine when regions of data are simply not present, but this data should come from differently-sized simulations because otherwise when making predictions for small areas, the model would be liable to hallucinate long range interactions from nonexistent sources because it would think that it is only predicting a small bit of a larger section of data.

- If we want to try to make it work with a variety of compounds (using attention over KV pairs from other (sufficiently similar) simulations using the unseen compounds to predict how the compounds act), a variety of compounds will probably need to be present in the data. This could be hard, so perhaps we could just run simulations with fictitious, randomized compounds that are just carbon chains/loops with random atoms attached. I'm sure this isn't ideal, real compounds are probably better. Also, assuming we are using learned embeddings, before training on an unseen compound, its embedding would have to be learned by backpropagating through the 1st level at least of the VAE.

- wait... should the learned embeddings associated with the residues be learned over individual frames??? that does not seem right, because the embedding should capture how the residue behaves... though the frames will be randomly sampled from simulation data, so even if time is not used when learning the data, the embedding would represent the contexts that the residue statistically tends to show up in. 

- Should use Monarch layers instead of dense layers, just because they are more parameter-efficient and a bit faster.
On that note, quantization sounds really nice, but methods I've seen only worsen training.
It seems like a nice idea to use a SOM (self-organizing map) instead of a VAE, mostly because the former is simpler and I understand it much better at the moment, but a transformer based VAE is a thing that makes sense, while a combination of the SOM and transformer architectures is perhaps not. An SOM could be used as a 'front-end' feature extractor, with a neural network on the back end... or... SOMs points are technically differentiable too, just like artificial neurons, so perhaps an SOM could be constructed as just a neural network with some weird extra dynamics, and the added notion of locality would enable it to start from a small network and then 'grow' by adding additional neurons in semi-reasonable places.
But attention heads normally use Q, K and V dimensions as high as 512, which is a *really* excessive number of dimensions for an SOM- I feel quite confident in saying a 512 dimensional SOM would not behave normally in any respect. That being said, artificial neurons have a tendency to be a little redundant in what they learn, a collection of SOMs could be used to the same effect as a collection of neurons, but that would not really be a beautiful solution, and I'm not sure how you would make sure the SOM outputs are decorrelated... high dimensionality and random initialization might take care of that. 
It is technically possible to use SOMs as attention heads, by assigning the value vectors to each point. I like that idea.
Not sure where the value vectors come from then, but I like the idea.


- I think that we should train a confidence module for the following reasons:
	- When training the model on longer timesteps from trajectories it has hallucinated, only high-quality data should be used
	- Also, for longer timesteps, it is infeasible for the model to predict the exact state found from MD (or bootstrapping) from extremely noisy inputs, because the dynamics are chaotic, and so there are multiple divergent trajectories possible from what are essentially the same initial state. A confidence model could be used to rate the output, and that can be used as a term in the loss function, so that the model still gets some credit for trajectories that are plausible but not exactly the one the MD simulation found. On that note, after a very long timestep is generated, an interpolation should probably be predicted, and then the confidence module should be used to evaluate the shorter jumps, and the error should be summed or something so that the model is trained on the cumulative error, because the confidence model is unlikely to be accurate for long timesteps for the same reason that the model is unlikely to be accurate for long timesteps. Then, the confidence model can be bootstrapped as well, so that both the confidence module and the prediction model learn to be accurate for progressively larger timesteps.
	- MD data is very computationally expensive, so an accurate prediction of the loss without a corresponding MD trajectory would be great.
	- It shouldn't be that expensive to train, given that it can use the same data, latent space, and such as the original model, and it only has to predict a single number.
- should we train a meta-confidence model???
	- it would be funny
	- if something's going wrong, it could help determine whether the confidence module is failing
- should we train a meta-meta confidence model??
	- it would be funny
	- probably not.